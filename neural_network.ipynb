{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP6PCJFK2w91R1rjB5/vsY+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nakajinbee/zero-tuku-deep-learning-2/blob/main/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm-ClMZVt0cd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 /(1 + np.exp(-x))\n",
        "\n",
        "x = np.random.randn(10, 2)\n",
        "w1 = np.random.randn(2, 4)\n",
        "b1 = np.random.randn(4)\n",
        "w2 = np.random.randn(4, 3)\n",
        "b2 = np.random.randn(3)\n",
        "\n",
        "h = np.dot(x,w1) + b1\n",
        "print(\"hの値：\")\n",
        "print(h)\n",
        "a = sigmoid(h)\n",
        "print(\"aの値：\")\n",
        "print(a)\n",
        "s = np.dot(a, w2) + b2\n",
        "print(\"sの値\")\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoidレイア \n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = 1/(1 + np.exp(-x))\n",
        "    self.out = out\n",
        "    return out\n",
        "    \n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 * self.out) * self.out\n",
        "    return dx"
      ],
      "metadata": {
        "id": "osDinUWrcdG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 全結合層レイアであるAffineレイア\n",
        "class Affine:\n",
        "  def __init__(self, w, b):\n",
        "    self.params = [w, b]\n",
        "    self.grads  = [np.zeros_like(w), np.zeros_like(b)]\n",
        "  \n",
        "  def forward(self,x):\n",
        "    w, b = self.params\n",
        "    out = np.dot(x, w) + b\n",
        "    self.x = x\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    w, b = self.params\n",
        "    dx = np.dot(dout, w.T)\n",
        "    dw = np.dot(self.x.T, dout) \n",
        "    db = np.sum(dout, axis=0)\n",
        "\n",
        "    self.grads[0][...] = dw\n",
        "    self.grads[1][...] = db\n",
        "    return dx\n",
        "    "
      ],
      "metadata": {
        "id": "7rr_QUCdd2w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    I, H, O = input_size, hidden_size, output_size\n",
        "\n",
        "    # 重みとバイアスの初期化\n",
        "    self.layers = [\n",
        "                   Affine(w1, b1),\n",
        "                   Sigmoid(),\n",
        "                   Affine(w2, b2)\n",
        "    ]\n",
        "\n",
        "    # 全ての重みをリストにまとめる\n",
        "    self.params = []\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params\n",
        "    \n",
        "  def predict(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer.forward(x)\n",
        "    return x\n",
        "  "
      ],
      "metadata": {
        "id": "p4uIyaHEeLFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randn(10, 2)\n",
        "model = TwoLayerNet(2, 4, 3)\n",
        "s = model.predict(x)\n",
        "\n",
        "print(s)"
      ],
      "metadata": {
        "id": "RJWOU6wFgCKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MatMul:\n",
        "  def __init__(w):\n",
        "    self.params = [w]\n",
        "    self.grads = [np.zeros_like(w)]\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    w, = self.params\n",
        "    out = np.dot(x,w)\n",
        "    self.x = x\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    w, = self.params\n",
        "    dx = np.dot(dout, w.T)\n",
        "    dw = np.dot(self.x.T, dout)\n",
        "    self.grads[0][...] = dw\n",
        "    return dx\n",
        "    "
      ],
      "metadata": {
        "id": "R790-3rlgiRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "\n",
        "  def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x - x.max(axis=1, keepdims=True)\n",
        "        x = np.exp(x)\n",
        "        x /= x.sum(axis=1, keepdims=True)\n",
        "    elif x.ndim == 1:\n",
        "        x = x - np.max(x)\n",
        "        x = np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = softmax(x)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = self.out * dout\n",
        "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
        "        dx -= self.out * sumdx\n",
        "        return dx"
      ],
      "metadata": {
        "id": "RE5tWmQ8yw2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1q94gufd6i2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "誤差逆伝播法によって勾配を求めた\n",
        "⇨勾配を用いてニューラルネットワークのパラメータを更新する\n",
        "\n",
        "step1 ミニバッチ：訓練データの中からランダムに複数のデータを選び出す\n",
        "\n",
        "step2 勾配の算出：誤差逆伝播法により、各重みパラメータに関する損失関数の勾配をもとめる\n",
        "\n",
        "step3 パラメータの更新：勾配を使って重みパラメータを更新する\n",
        "\n",
        "step4 繰り返す：１２３を必要な回数だけ繰り返す"
      ],
      "metadata": {
        "id": "ZMAduXyo6kKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SDG : Stochastic Gradient Decent (確率的勾配降下法)\n",
        "# lr : learning rate 学習係数\n",
        "\n",
        "class SGD:\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr = lr\n",
        "  \n",
        "  def update(self, params, grads):\n",
        "    for i in range(len(params)):\n",
        "      params[i] -= self.lr * grads[i]\n",
        "  "
      ],
      "metadata": {
        "id": "k3rJj20R7UKI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}